# -*- coding: utf-8 -*-
"""Kopia notatnika 21poprawiony.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NTdm2ZB__KJEZC2aGPzRB1KdKtFwiQfZ
"""

! pip install transformers datasets evaluate pyarrow==14.0.1

from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments
from evaluate import evaluator
from datasets import load_dataset, load_metric
import numpy as np # Import numpy for argmax

# Załaduj dane z podziałem na treningowy i walidacyjny
# Assign the splits to separate variables
train_dataset, eval_dataset = load_dataset('text', data_files={'train': '/content/sample_data/train.txt'}, split=['train[:80%]', 'train[80%:]'] )

# Inicjalizacja tokenizer'a i modelu
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# Funkcja do przypisania etykiet (popraw na podstawie swoich danych)
def add_labels(example):
    if len(example['text']) > 10:
        example['labels'] = 1
    else:
        example['labels'] = 0
    return example

# Apply the add_labels function to both training and validation splits
train_dataset = train_dataset.map(add_labels)
eval_dataset = eval_dataset.map(add_labels)

def tokenize_function(examples):
    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True)
    tokenized_output["labels"] = examples["labels"]
    return tokenized_output

tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)

# Parametry fine-tuningu
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=1e-5,
    eval_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,  # Loguj stratę co 10 kroków
    evaluation_strategy="epoch" # Dodane - Rafał - aby wyświetlało training loss
)

# Metryka
metric = load_metric("accuracy")

# Funkcja do obliczania metryk
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Inicjalizacja i uruchomienie fine-tuningu
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset, # Use the tokenized training dataset
    eval_dataset=tokenized_eval_dataset, # Use the tokenized validation dataset
    compute_metrics=compute_metrics,
)

trainer.train()

# Ewaluacja modelu
eval_results = trainer.evaluate()
print(f"Wyniki ewaluacji: {eval_results}")